---
title: "Getting Started with Genomic Prediction"
author: "Julin Maloof"
date: "Updated March 21-25, 2017"
output: 
  ioslides_presentation:
    center: false
    widescreen: true
    fig_width: 6
    fig_height: 4
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE,autodep = TRUE,eval=FALSE)
```

```{r libraries, include=FALSE, eval=TRUE, cache=FALSE}
library(bigRR)
library(ggplot2)
library(magrittr)
library(reshape2)
library(rrBLUP)
library(BGLR)
library(gridExtra)
library(snowfall)
#load("Genome_Prediction_Fits.Rdata")
```

```{r, include=FALSE, eval=TRUE, cache=TRUE}

SubsetBigRR <- function(fit,proportion) {
  which(abs(fit$u) > quantile(abs(fit$u),1-proportion))
}

FitBigRR <- function(phenotype, genotype) {
  
  bigRR1 <- bigRR(y=phenotype,
                  X = matrix(1,nrow = length(phenotype),ncol=1), # intercepts
                  Z = genotype) #should be centered
  
  #this allows heteroscedatic shrinkage (stronger shrinkage on some SNPs than others).
  bigRR1.update <- bigRR_update(bigRR1,Z = genotype) 
  return(bigRR1.update)
} 

PredictBigRR <- function(model,genotype,geno.subset=NA) {
  # given a bigRR model and a list of genotypes, compute predicted phenotype
  if(is.na(geno.subset[1])) {
    prediction <- model$beta + genotype %*% model$u
  } else {
    prediction <- model$beta + genotype[,geno.subset] %*% model$u[geno.subset,]
  }
  return(prediction)
}

FitBayesC <- function(phenotype, genotype, geno.subset) {
  
}

PredictBGLR <- function(fit, genotype) {
  
}

PlotPrediction <- function(prediction.new,prediction.original,phenotype,optimize) {
  if (optimize=='max') x=max(prediction.new) else x=min(prediction.new)
  if (is.na(prediction.original)) {
    plot.df <- as.data.frame(prediction.new)
    plot.df$type <- "new"
  } else {
    plot.df <- as.data.frame(rbind(prediction.new,prediction.original))
    plot.df$type <- rep(c("cross","original"), c(nrow(prediction.new),nrow(prediction.original)))
  }
  print(head(plot.df))
  pl <- ggplot(plot.df,aes_(x=as.name(phenotype),fill=~type))
  pl <- pl + geom_density(alpha=0.5)
  pl <- pl + geom_segment(x=x,xend=x,y=.1,yend=0,col="blue",size=1.5,arrow=arrow())
  pl + ggtitle("Distribution of Phenotypes")
}
```

## RMarkdown presentation

This presentation made in R Markdown.

The file is available at:

https://github.com/jnmaloof/Hypocotyl_Genomic_Selection


# Genomic Selection Intro

## Goal: Predictive Breeding

* Want to breed plants for better performance, lipid production, etc.
* Use genetic / genomic data to predict best plants for breeding

## Old school: Marker assisted selection {.build}

* Use QTL or GWAS to find markers linked to trait of interest
* Genotype at those markers to "pyramid" during breeding
* Problem: ignores the many, many small loci that contribute to the trait

## Genomic Selection {.build}

* Instead of focusing on a few main loci, try to predict the effects of all loci
* _Old:_ QTL and GWAS fit a separate regression model for each location being considered

$oil \sim \beta0 + \beta1*M1$

$oil \sim \beta0 + \beta2*M2$

$oil \sim \beta0 + \beta3*M3$

...

$oil \sim  \beta * M250000$

* _New:_ Genomic Selection fits a model to all markers simultaneously

$oil \sim \beta1*M1 + \beta2*M2 + \beta3*M3 + ... +\beta250000*M250000$

## Genomic Selection {.build}

* Genomic Selection fits a model to all markers simultaneously

$oil \sim \beta1*M1 + \beta2*M2 + \beta3*M3 + ... +\beta250000*M250000$

* Once this model is fit then we can predict the performance of other strains
* Challenge: How to fit a regression model with hundreds of thousands of predictors?

## Fitting a model (standard regression) {.build}

What is the best value for $\beta$?

```{r, fig.width=2,fig.height=2,echo=FALSE, eval=TRUE}
data <- data.frame(
  genotype = rep(c(0,1,2),c(10,10,10)),
  oil = rnorm(30,rep(c(10,20,30),c(10,10,10)),sd = 4))
pl <- ggplot(data,aes(x=genotype,y=oil)) + geom_point() + geom_smooth(method="lm",level=.99)
pl + scale_x_continuous(breaks = 0:2, labels=c("0 (aa)","1 (aA)","2 (AA)")) + ylab("oil content") + xlab("M1 genotype")
```

In standard regression with one or just a few predictors, $oil \sim \beta1*M1$, choose the $\beta$ that minimizes the residual sum of squares (RSS), i.e the distance from the line
$$ RSS =  \sum_{i=1}^n (y_i - \beta_0 + \beta_1*M1_i )^2$$

## RSS more than one predictor {.build}

Instead of

$$ RSS =  \sum_{i=1}^n (y_i - \beta_0 + \beta_1*M1_i )^2$$

For more than one predictor, we can generalize as
$$ RSS =  \sum_{i=1}^n (y_i - \beta_0 + \sum_{j=1}^p\beta_j*M_{i,j} )^2 $$

## Fitting  model (penalized regression) {.build}

* Standard regression models do not perform well with many predictors
* If we expect that most predictors will have an effect size of ~ 0 (we do in this case), use _penalized regression_
* Penalized regression _penalizes_ models based on the number of predictors.  For example (lasso), minimize:
$$  \sum_{i=1}^n (y_i - \beta_0 + \sum_{j=1}^p\beta_j*M_{i,j} )^2 + \lambda \sum_{j=1}^p|\beta_j| $$

$$ = RSS + \lambda \sum_{j=1}^p|\beta_j|$$

* Where $\lambda$ is a tuning parameter
* "Penalize" models for non-zero coefficients
* Other versions include Ridge Regression, Elastic Net

## Bayesian Penalized regression

* Use priors strongly biased towards zero
* Horseshoe, Laplace, Student's T, etc.

# Examples

## Tool exploration

* Use hypocotyl data set from Filiault and Maloof (2012)
* 169 Arabidopsis natural accessions
* 250,000 SNPs

```{r, include=FALSE, eval=TRUE}
## get the data

#Also center the genotypes, so that the genotypes are represented as -1, 0, +1, as needed by some packages.

load("pheno.geno.Rdata")
geno.cols <- grep("^V",colnames(pheno.geno))
geno.center <- geno*2-1
head(geno.center[,1:10])
head(geno[,1:10])
all(row.names(geno.center) == row.names(pheno))
pheno <- as.data.frame(pheno)
```


# bigRR

## bigRR

* Package bigRR (big Ridge Regression) uses optimized code to fit penalized Ridge Regression models.  
* Enables separate shrinkage parameters to be used for each marker, in a two-step process

```{r, include=FALSE, eval=TRUE}
bigRR1 <- FitBigRR(phenotype = pheno$high.RFR,genotype = geno.center)
```

## Two steps of model validation

We need to test the genomic prediction models that we create:

1) Compare fitted data with observations.  Can the model reproduce the observations used to create the model?

2) Split the data into "training" and "test" sets.  

## BigRR Fitted vs original, all data

```{r, echo=FALSE, eval=TRUE}
prediction.original <- PredictBigRR(bigRR1,geno)

cor.original <- cor(pheno$high.RFR,prediction.original)

pl.fit <- qplot(x = pheno$high.RFR,y=prediction.original,geom = "point")
pl.fit <- pl.fit + xlab("Original") + ylab("Fitted") + ggtitle(paste("Fitted vs. Original, All Data; correlation: ",round(cor.original,4)))

pl.fit
```

So at least the model can reproduce the origninal data.

## predict new observations

Here create 100 training / test sets where the training sets are a randomly picked 75% of the lines and the test sets are the remaining 25%.

```{r, include = FALSE, eval=FALSE}
percent <- 75 # size of training set
iterations <- 100
phenotype <- pheno$high.RFR
train.list <- lapply(1:iterations,function(x) sample(1:length(phenotype),size = round(length(phenotype)*percent/100), replace = FALSE))
test.list <- lapply(train.list, function(x) setdiff(1:length(phenotype),x))

sfInit(parallel = TRUE, cpus = 4)
sfLibrary(bigRR)
sfExport("percent","geno.center","phenotype","FitBigRR","PredictBigRR","train.list", "test.list")
CV.trials <- sfLapply(1:iterations,function(trial) {
  fit.tmp <- FitBigRR(phenotype=phenotype[train.list[[trial]]],
                      genotype = geno.center[train.list[[trial]],])
  data.frame(observed=phenotype[test.list[[trial]]],
             predicted=PredictBigRR(fit.tmp,genotype = geno.center[test.list[[trial]],]))
})
sfStop()

corrs <- data.frame(Pearson=sapply(CV.trials,function(trial) c(Pearson=cor(trial)[1,2])))
corrs.m <- melt(corrs,variable.name="method",value.name="correlation")

pl.cor <- ggplot(corrs.m,aes(x=method,y=correlation)) +
  geom_boxplot() +
  ggtitle("Summary of Original vs. Predicted \n correlation in CV trials")

example.trial <- which(abs(corrs$Pearson-median(corrs$Pearson)) ==
                         min(abs(corrs$Pearson-median(corrs$Pearson))))[1]

pl.CV.example <- qplot(
  x=CV.trials[[example.trial]]$observed,
  y=CV.trials[[example.trial]]$predicted,geom="point")
pl.CV.example <- pl.CV.example + 
  xlab("Original") + 
  ylab("Predicted") + ggtitle("Predicted vs. Original, median CV Trial")
save.image("Mar2017_Genome_Prediction_Fits.Rdata")
```

```{r, eval=TRUE, echo=FALSE}
load("Mar2017_Genome_Prediction_Fits.Rdata")
grid.arrange(pl.cor,pl.CV.example,ncol=2, widths=c(3,5))
```


## how many SNPs do we need? {.build}

For doing predictions of new crosses it would be nice to not use *all* of the markers (for computational efficiency).  Test if top 10% of markers is sufficient.

First for the full model
```{r, echo=FALSE, eval=TRUE, include=FALSE}

geno.subset <- SubsetBigRR(bigRR1,.1)

bigRR2 <- FitBigRR(pheno$high.RFR,geno.center[,geno.subset])

prediction.bigRR2 <- PredictBigRR(bigRR2,geno.center[,geno.subset])
```

```{r, echo=FALSE, eval=TRUE}

predict.pl.df <- data.frame(
  observed=c(pheno$high.RFR,pheno$high.RFR),
  fitted=c(prediction.original,prediction.bigRR2),
  data=rep(c("Full","Best 10% Markers"),each=length(prediction.original))
)

pl <- ggplot(predict.pl.df,aes(x=observed,y=fitted,color=data))
pl + geom_point() + ggtitle("original vs fitted")
```

The fit is actually _improved_ by using only the best markers.

## predict new observations using only top 10% {.build}

How does reducing the number of markers affect prediction of new observations?  Try this on the "median" training/test set from above

```{r, include=FALSE, eval=TRUE}

bigRR3 <- FitBigRR(phenotype=phenotype[train.list[[example.trial]]],
                      genotype = geno.center[train.list[[example.trial]],])

prediction.bigRR3 <- PredictBigRR(bigRR3,geno.center[test.list[[example.trial]],])

geno.subset <- SubsetBigRR(bigRR3,.1)

bigRR4 <- FitBigRR(phenotype=phenotype[train.list[[example.trial]]],
                      genotype = geno.center[train.list[[example.trial]],geno.subset])

prediction.bigRR4 <- PredictBigRR(bigRR4,geno.center[test.list[[example.trial]],geno.subset])
```

```{r echo=FALSE, eval=TRUE}
predict.pl.df <- data.frame(
  observed=rep(phenotype[test.list[[example.trial]]],2),
  predicted=c(prediction.bigRR3,prediction.bigRR4),
  data=rep(c("Full","Best 10% Markers"),each=length(prediction.bigRR3))
)

pl <- ggplot(predict.pl.df,aes(x=observed,y=predicted,color=data))
pl + geom_point() + ggtitle("original vs predicted")
```

## predict new observations using only top 10%

```{r, echo=FALSE, eval=TRUE}
knitr::kable(round(cor(data.frame(observed=phenotype[test.list[[example.trial]]],
                                   predicted.full=prediction.bigRR3,
                                   predicted.10.percent=prediction.bigRR4)),3))
```

Conclusion: reducing to the best 10% of markers does not harm the predictions

# rrBLUP

## rrBLUP
similar to bigRR, not included

```{r, eval=FALSE, include=FALSE} 
# not needed for this analysis
impute <- A.mat(geno.center,max.missing = .5, impute.method = "mean", return.imputed = TRUE,n.core=3)
geno_impute <- impute$imputed
any(apply(geno_impute,2,function(x) any(is.na(x)))) #check for NAs.  If there are any, then would need to remove NA columns
```

fit model
```{r, eval= FALSE, include=FALSE}
rrBLUP1 <- mixed.solve(pheno.train$high.RFR, Z=geno.train)
names(rrBLUP1)
rrBLUP1$beta
head(rrBLUP1$u)
```

predict from model
```{r, eval=FALSE, include=FALSE}
prediction <- rrBLUP1$beta[1] + geno.test %*% as.matrix(rrBLUP1$u)
cor(pheno.test$high.RFR,prediction)
plot(pheno.test$high.RFR,prediction)
```

# BGLR

## BGLR 

BGLR is a package that implements many different Bayesian approached to genomics selection.

Two versions will be explored here
* Bayesian Lasso
* Bayes C

Will fit with and without a population structure correction, K

```{r, eval=TRUE}
load("Genome_Prediction_Fits.Rdata")
```


```{r, include=FALSE}
system.time(bglr.bl1 <- BGLR(y=pheno.train.120$high.RFR, 
                             ETA=list(list(X=geno.train.120,model="BL")),
                             verbose = FALSE,
                             saveAt = "bglr_output/bglr.bl1_",
                             niter=12000,burnIn = 2000))
```

Fit a Bayesian Lasso model, with K

```{r, include=FALSE}
system.time(bglr.bl2 <- BGLR(y=pheno.train.120$high.RFR,
                             ETA=list(list(X=geno.train.120,model="BL"),
                                      list(K=K.mat.train.120,model="RKHS")),
                             verbose = FALSE,
                             R2=.75,
                             saveAt = "bglr_output/bglr.bl2.R2.75_",
                             nIter=12000,burnIn = 2000))

```

Fit a BayesC model with

```{r, include=FALSE}
system.time(bglr.bc1 <- BGLR(y=pheno.train.120$high.RFR, 
                             ETA=list(list(X=geno.train.120,model="BayesC")),
                             verbose = FALSE,
                             saveAt = "bglr_output/bglr.bc1_",
                             nIter=12000,burnIn = 2000))
```

Fit a BayesC model with K
```{r, include=FALSE}

system.time(bglr.bc2 <- BGLR(y=pheno.train.120$high.RFR,
                             ETA=list(list(X=geno.train.120,model="BayesC"),
                                      list(K=K.mat.train.120,model="RKHS")),
                             verbose = FALSE,
                             saveAt = "bglr_output/bglr.bc2_",
                             nIter=12000,burnIn = 2000))
```

```{r, eval=FALSE, include=FALSE}
save.image(file = "Genome_Prediction_Fits.Rdata")
```

```{r bglr_functions, include=FALSE, eval=TRUE}
plot_fitted_observed <- function(fit,title="fitted vs observed, training set") {
  print(qplot(pheno.train.120$high.RFR,fit$yHat,geom = "point",main = title,xlab="observed",ylab="fitted"))
  c(cor(pheno.train.120$high.RFR,fit$yHat),cor(pheno.train.120$high.RFR,fit$yHat,method="kendall"))
}

plot_predicted_observed <- function(fit,title="predicted vs observed, test set") {
  prediction <- geno.test.120 %*% fit$ETA[[1]]$b + fit$mu
  print(qplot(pheno.test.120$high.RFR,prediction,xlab='observed',ylab='predicted',main=title,geom="point"))
  c(cor(prediction,pheno.test.120$high.RFR),cor(prediction,pheno.test.120$high.RFR,method = "kendall"))
}
```

## Bayesian Lasso, no structure, fit

```{r, eval=TRUE, echo=FALSE}
plot_fitted_observed(bglr.bl1)
```

## Bayesian Lasso, no structure, predict

```{r, eval= TRUE, echo=FALSE}
plot_predicted_observed(bglr.bl1)
```

## Bayesian Lasso, with structure, fit

```{r, eval=TRUE, echo=FALSE}
plot_fitted_observed(bglr.bl2)
```

## Bayesian Lasso, with structure, predict

```{r, eval=TRUE, echo=FALSE}
plot_predicted_observed(bglr.bl2)
```

## BayesC, no structure, fit

```{r, eval=TRUE, echo=FALSE}
plot_fitted_observed(bglr.bc1)
```

## BayesC, no structure, predict

```{r, eval=TRUE, echo=FALSE}
plot_predicted_observed(bglr.bc1)
```

## BayesC, with structure, fit

```{r, eval=TRUE, echo=FALSE}
plot_fitted_observed(bglr.bc2)
```

## BayesC, with structure, predict

```{r, eval=TRUE, echo=FALSE}
plot_predicted_observed(bglr.bc2)
```

```{r,eval=FALSE,echo=FALSE}
#3# Goodness of fit and related statistics
bglr.bl1$fit
blgr.bl1$varE # compare to var(y)

#4# Trace plots
list.files("bglr_output")

# Residual variance
varE<-scan('varE.dat') 
plot(varE,type='o',col=2,cex=.5,ylab=expression(var[e])); abline(h=bglr.bl1$varE,col=4,lwd=2); abline(v=bglr.bl1$burnIn/bglr.bl1$thin,col=4)
# lambda (regularization parameter of the Bayesian Lasso)
lambda<-scan('ETA_1_lambda.dat')
plot(lambda,type='o',col=2,cex=.5,ylab=expression(lambda)); abline(h=bglr.bl1$ETA[[1]]$lambda,col=4,lwd=2); abline(v=bglr.bl1$burnIn/bglr.bl1$thin,col=4)
```

Residual Variance
```{r}
varE.files <- dir("bglr_output",pattern="varE.dat",full.names = TRUE)
for (f in varE.files) {
  tmp.varE <- scan(f)
  plot(tmp.varE,type='l',col=2,cex=.5,ylab=expression(var[e]),main=basename(f))
  abline(h=bglr.bl1$varE,col=4,lwd=2); abline(v=bglr.bl1$burnIn/bglr.bl1$thin,col=4)
}
```
These are clearly autcorrelatd and not mixing that well...

# multiple test and training sets

## multiple test and training sets

pull training and test sets at random and check performance both of bigRR and BGLR

first create lists of random train and test sets.  training size is 120

```{r}
n.trials <- 100
train.list <- lapply(1:n.trials,function(x) sample(1:nrow(pheno),size = 120, replace = FALSE))
test.list <- lapply(train.list, function(x) setdiff(1:nrow(pheno),x))
```

## run bigRR and record correlations

```{r}
bigRR.120.100trials <- sapply(1:n.trials,function(trial) {
  bigRR.train.tmp <- bigRR(y=pheno$high.RFR[train.list[[trial]]],
                           X = matrix(1,nrow = length(train.list[[trial]]),ncol=1), # intercepts
                           Z = geno[train.list[[trial]],])
  bigRR.train.tmp.update <- bigRR_update(bigRR.train.tmp,Z =  geno[train.list[[trial]],]) 
  tmp.predict <- data.frame(observed=pheno$high.RFR[test.list[[trial]]],
                            predicted=bigRR.train.tmp.update$beta + geno[test.list[[trial]],] %*% bigRR.train.tmp.update$u)
  c(Pearson=cor(tmp.predict)[1,2],Kendall=cor(tmp.predict,method="kendall")[1,2])
})
save.image(file = "Genome_Prediction_Fits.Rdata")
summary(t(bigRR.120.100trials))
```

## Bayesian Lasso Loop

```{r, eval=FALSE}
library(snowfall)
sfInit(cpus = 2)
bglr.bl.120.100trials <- sfApply(1:n.trials,function(trial) {
  
  bl.tmp <-  BGLR(y=pheno$high.RFR[train.list[[trial]]],
                  ETA=list(list(X=geno[train.list[[trial]],],model="BL"),
                           list(K=K.mat[train.list[[trial]],train.list[[trial]]],model="RKHS")),
                  verbose = FALSE,
                  nIter=5000,burnIn = 1000)
  prediction <- geno[test.list[[trial]],] %*% bl.tmp$ETA[[1]]$b + bl.tmp$mu
  
  c(Pearson=cor(prediction,pheno.test.120$high.RFR)[1,2],Kendall=cor(prediction,pheno.test.120$high.RFR,method = "kendall")[1,2])
  save.image(file = "Genome_Prediction_Fits.Rdata")
})
summary(bglr.bl.120.100trials)
```


## BayesC Loop

```{r, eval=FALSE}
library(snowfall)
sfInit(parallel=TRUE,cpus = 2)
sfLibrary(BGLR)
sfExport("n.trials","test.list","train.list","geno","pheno","K.mat")
bglr.bc.120.100trials <- sfSapply(1:n.trials,function(trial) {
  print(trial)
  
  bl.tmp <-  BGLR(y=pheno$high.RFR[train.list[[trial]]],
                  ETA=list(list(X=geno[train.list[[trial]],],model="BayesC"),
                           list(K=K.mat[train.list[[trial]],train.list[[trial]]],model="RKHS")),
                  verbose = FALSE,
                  nIter=5000,burnIn = 1000)
  tmp.predict <- data.frame(observed=pheno$high.RFR[test.list[[trial]]],
                            predicted=geno[test.list[[trial]],] %*% bl.tmp$ETA[[1]]$b + bl.tmp$mu)
  
  c(Pearson=cor(tmp.predict)[1,2],Kendall=cor(tmp.predict,method="kendall")[1,2])
})
save.image(file = "Genome_Prediction_Fits.Rdata")
summary(t(bglr.bc.120.100trials))
```

## BayesC and bigRR
100 trials with a training set of 120
```{r}
load("Genome_Prediction_Fits.Rdata")
```

```{r}
plot(bigRR.120.100trials["Pearson",],bglr.bc.120.100trials["Pearson",],xlab="bigRR",ylab="BLGR BayesC")
lines(x=c(.1,.8),y=c(.1,.8))
sum(bigRR.120.100trials["Pearson",]>bglr.bc.120.100trials["Pearson",])
```
For the Arabidopsis data set these two methods perform very similarly, so might as well use the faster bigRR

# To Do

## To do

* Determine effect of training set size
* Check BGLR convergence
* Compare to traditional GWAS
* Marker effects
* Test crosses